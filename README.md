# Description

This repository extends rpryzant@stanford.edu's implementation of the DeleteOnly and DeleteAndRetrieve models from [Delete, Retrieve, Generate:
A Simple Approach to Sentiment and Style Transfer](https://arxiv.org/pdf/1804.06437.pdf). Specifically it adds:

* adds backtranslation approach from https://research.fb.com/wp-content/uploads/2019/04/Multiple-Attribute-Text-Rewriting.pdf?
* adds Transformer encoder and decoder modules
* adds CNN style discriminators inspired by https://arxiv.org/pdf/1705.09655.pdf (primarily) and https://arxiv.org/abs/1703.00955 and adversarial training loop
    * discriminators differentiate between the hidden states of two sequences generated by the decoder. One sequence is teacher-forced from target training sequences. The other sequence is generated from a soft probability distribution over tokens that allows for backprop. 
* refactored to support an arbitrary number of styles during training / inference, and multi-style averaging during inference (both inspired by https://research.fb.com/wp-content/uploads/2019/04/Multiple-Attribute-Text-Rewriting.pdf?)
* integrates generic noising, word attribute selection, and ngram attribute selection in data preparation functions
* adds top k decoding option
* adds fused language model and GPT, GPT2 tokenizers
* adds additional loss function: lower bound on expected bleu score
* adds backtran

# Installation

`pip install -r requirements.txt`

This code uses python 3. 

`flask.dockerfile` generates flask app docker image that processes request
`train.dockerfile` generates docker image that can be used for training

# Usage

### Training

`python train.py --config yelp_config.json --bleu`

Examples of _delete_ model on a dataset of yelp reviews:

![curves](https://i.imgur.com/jfYaDBr.png)


Checkpoints, logs, model outputs, and TensorBoard summaries are written to the `/checkpoints/` + the config's `working_dir`.

See `config.json` for all of the training options. Important parameters include `model_type` (`delete`, `delete_retrieve`, or `seq2seq` (which is a standard translation-style model)), `bt_ratio` (loss ratio of back-translated samples), `discriminator_ratio` (loss ratio of discriminator losses, one per style, summed), `tokenizer` (gpt, gpt2), `encoder/decoder` (lstm, transformer), `decode` (greedy, top k)

# Questions, feedback, bugs

jeffrey.gleason@newknowledge.io

# Original Developer:

rpryzant@stanford.edu

# Original Acknowledgements

Thanks lots to [Karishma Mandyam](https://github.com/kmandyam) for contributing! 

